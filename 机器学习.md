# <span style="color: #eb52f7">机器学习</span>

**无监督学习**

定义：样本无输出，自定义划分样本为若干子集。

## K均值聚类

迭代求解策略，按距离分配，K个聚类中心 。

## 模糊C均值

柔性划分，改进型K均值聚类，适用于含有噪声的聚类任务

## 谱聚类算法

聚类问题转化为图的最优划分问题

# 评价指标

## 分类算法的评价指标

### 准确率

整体性能的平均

### 精确率和召回率

每个类进行性能估计

精确率：被预测是对的样本中，确实是对的样本所占比例。

召回率：在确实是对的样本中，被预测是对的样本所占比例。

### F值

精确率和召回率的调和平均

### 交叉验证

原始数据评分为K组，留下一组作为验证集。可以由K次实验得到K个模型。K个模型在各自验证集上的错误率平均作为分类器的评价。

## 聚类算法的评价指标

### 性能度量

有效性指标（用作评价好坏指标和最终优目标）

外部指标：JC系数、FM指数、Random指数，结果均在\[0-1]之间，越大越好

内部指标：DBI指数、DI指数。DBI越小越好，DI越大越好。

### 距离计算

距离越小，相似度越大。

# 机器学习技术应用

分类技术——数据集合进行模型训练，测试数据分类；适用于有标注数据集合

回归技术——预测输出

聚类算法——将无规律信息整合成有一定规则的组织结构；无标注数据集合。

# 感知机（两层神经元结构：外层输入→输入→输出（M-P神经元)）

感知机学习就是**选取使损失函数最小的模型参数；最小化错误分类样本到超平面的距离**。

<span style="color: rgb(78, 179, 28)">损失函数来源于误分类点到分离超平面的距离之和。</span>

将权重向量和偏置看作一个超平面S的参数。

<span style="color: rgb(78, 179, 28)">错误分类样本离超平面越近，损失函数值越小。</span>

## 感知机学习算法

### <span style="color: rgb(255, 32, 32)">目标：损失函数极小化</span>

方法：<span style="color: rgb(78, 179, 28)">随机梯度下降法</span>

### <span style="color: #ffcb00">二分类感知机学习算法</span>

对于线性可分的数据集，可以利用<span style="color: rgb(78, 179, 28)">有限次迭代学习</span>将一个数据集完全正确地分类或达到收敛条件。

注：<span style="color: rgb(78, 179, 28)">个人感觉就是线性规划，类似利用函数，去框柱数据点的边界，然后进行分类。</span>（图2.10.c）

处理多分类问题时，在输入输出联合空间上构建一个特征函数，样本映射到特征向量空间去处理复杂的输出，构建一个广义的感知机模型。

### 感知机学习的劣势

（1）不能保证超平面S在测试样本中的泛化能力最优，会过拟合。

（2）样本选择顺序不一样，学习的感知机模型也不一样。若要得到唯一解，需要进行约束条件。

（3）训练集不是线性可分的时候，感知机学习算法不会收敛，迭代结果会产生振荡。

## Python：利用sklearn实现感知机

sklearn：机器学习库，从数据预处理到训练模型的各个方面。还有一些自带的数据集。

# logistic回归（logistic regression，LR）

经典<span style="color: rgb(255, 32, 32)">分类方法，对数线性模型</span>。**<span style="color: rgb(255, 32, 32)">逻辑回归</span>**

## 优点：①训练速度快；②计算量取决于特征数；③可解释性好④适合<span style="color: rgb(255, 32, 32)">二分类问题</span>，不需要缩放输入特征；⑤内存资源占用小，只需要存储各个维度的特征值。

## 缺点：无法解决非线性问题；很难处理数据不平衡问题；难拟合真实数据分布。

## <span style="color: #ffcb00">logistic回归模型</span>（≈无约束最优化问题）

Logistic函数是一条S形曲线；

由**条件概率分布p(y|x）**——后验概率表示，x取值全体实数，y取值0或1；

Logistic回归比较两个条件概率值p( 1 | x ）和p( 0 | x ）的大小，将示例x分到概率值大的那一类。

几率=p( 1 | x ）/p( 0 | x ）事件发生概率和事件不发生概率的比值

几率的对数就是对数几率；

### Logistic回归定义：输出y=1的对数几率是由输入x的线性函数表示的模型。

## 模型参数估计

### 估计参数：极大似然估计法

### 优化参数：<span style="color: rgb(255, 32, 32)">梯度下降法</span>（<span style="color: rgb(5, 162, 239)">求解最优化问题常用算法</span>）,牛顿法

### 回归系数w求解（<span style="color: rgb(5, 162, 239)">对数似然函数</span>为目标函数的<span style="color: rgb(5, 162, 239)">最优化问题</span>）

方法：迭代算法

似然函数式取最大值，将其确定为目标函数，利用**上升算法**求解回归系数w的估计值。

**梯度下降法**优化求解最小值问题。——损失函数 L(w) 对w求偏导

w在微弱上升，b在下降。

## <span style="color: #ffcb00">多项Logistic回归</span>（Softmax回归模型）

由Logistic回归推广，用于多类分类，而不仅仅局限于二类分类。

把二元逻辑回归的单个权向量，拓展成一个权值矩阵W

## Python：利用sklearn建立Logistic回归模型

# 支持向量机（support vector machine，SVM）

**二分类的监督学习方法；决策边界是对学习样本求解的最大间隔超平面；带约束的最优化问题**。

SVM使用Hinge损失函数计算经验风险（empirical risk），在求解系统中加入了正则化项优化结构风险（structural risk）。

### 分类超平面有唯一解

<span style="color: rgb(78, 179, 28)">数据集线性可分时，划分样例的超平面有无穷多个。</span>

原因：在一个二维空间（可以参考《机器学习，胡清华，杨柳编著》图4.1）中，当数据被一条直线分开时，我们可以在保持分类不变的前提下，通过旋转或平移这条直线，生成无数个新的划分直线。同样地，在更高维度的空间中，我们也可以通过调整超平面的角度和位置，在不改变分类结果的情况下，得到无数个划分超平面。只要新的超平面能够将两类样本完全分开，并且分类结果与原始超平面一致，那么它就是一个有效的划分超平面。

<span style="color: rgb(255, 203, 0)">SVM 正确划分正负样例的基础上，增加了正负样例间的最大约束，因此超平面有唯一解。</span>

### SVM 通过核分类（kernel method）实现非线性分类

## 发展

### 基于松弛变量的规划问题和VC维的提出，SVM 理论逐步完善。

### 优化目标 ：结构风险（structural risk）最小化

**适应条件：<span style="color: #4eb31c">样本量少</span>，为了提高学习机泛化能力，实现empirical  risk和置信范围的最小化。**

非线性SVM——1992年

## <span style="color: #ffcb00">硬间隔支持向量机</span>（硬间隔不允许有违例样本）

### SVM 目标：<span style="color: #ff2020">间隔最大化</span>求解最优超平面。具有唯一解。

支持向量：在正负超平面上的点。决定了超平面的解。

间隔：两个不同类的支持向量到超平面的距离之和；正负超平面的距离。

**<span style="color: #ffcb00">支持向量个数远少于训练样本数。</span>**

### 参数学习

优化问题转为对偶问题；当对α求极大时候，面临一个二次规划问题，通过规划算法来求解，或者高效算法，例如<span style="color: #ffcb00">序列最小优化</span>（sequential minimal optimization，SMO ）

**<span style="color: #ffcb00">SMO：</span>**固定α^(n)之外的所有参数，求α^(n)上的极值，由其他变量表示α^(n)。 α^(j)也是如此操作；参数初始化后，不断选取需要更新的变量α^(n)和α^(j)，固定α^(n)和α^(j)以外的参数，求解获得更新后的α^(n)和α^(j)。α^(n)和α^(j)有一个不满足KKT条件，目标函数在迭代后减小。

## <span style="color: #ffcb00">软间隔线性SVM</span>（软间隔允许有违例样本，即异样点，相当于不考虑异样点的硬间隔线性SVM）

注：支持向量机实质是求解一个二次规划问题。

空间复杂度为O（N^2）；空间复杂度为O（N^3）。训练时间长，规模太大。

支持向量数也不能太多；

已经训练好的支持向量机无法将新增加的样本纳入。

## <span style="color: #ffcb00">非线性SVM</span>

### 特点：利用核技巧（kernel trick）

样本从原始空间映射到另一个特征空间，使样本在这个特征空间内线性可分。

如果原始空间是有限维度，那么一定存在一个高维特征空间使得样本分开。

将非线性问题转变为线性问题，进行求解。——非线性变换

常用核函数：线性核函数，多项式核函数，高斯核函数，字符串核函数

## <span style="color: #ffcb00">多核学习（multiple kernel learning,MKL)</span>

增强决策函数的可解释性，提升性能；效率不高。

高维空间由多个特征空间组合而成，充分发挥各个基本核的特征映射能力，也可以解决异构数据中不同特征分量的问题。

构建多核模型常用基本核函数的凸组合。

核心问题：基本核和权系数的优化。

时间复杂度：将传统求解方法转化为求解半定规划优化问题（SDP）

### 研究方向：核函数权系数选择问题和多核学习理论

## 支持向量机处理大规模问题的方法

工作集方法，并行方法，避免求解二次规划问题，几何方法，减少训练样本、训练集分解法、增量学习法。

# 神经网络（neural，network，NN）

连接主义模型，各个神经元之间的连接权重就是学习的参数。高度非线性模型。

基本单元：非线性激活函数的神经元。

整体结构：大量神经元之间的连接。
